{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP4_AI8cH1oN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision.transforms import Compose, ToTensor, Resize, RandomHorizontalFlip, GaussianBlur\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from PIL import ImageFilter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZjCSpXkIi-x",
        "outputId": "d2fe6539-524f-462e-e94e-0e750afd9035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmThrDtNInai",
        "outputId": "b97848bd-cfe9-4a9a-e539-446cba8526e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sLjCJhsOH1oW"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# -----------------------------------------------------------------------------------\n",
        "# SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257\n",
        "# Originally Written by Ze Liu, Modified by Jingyun Liang.\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "# We need the code for the SwinIR Model. Copying that file will give us a Pytorch model called SwinIR\n",
        "# We can load the weights that we downloaded above into the SwinIR Object\n",
        "\n",
        "import math\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
        "\n",
        "    def flops(self, N):\n",
        "        # calculate flops for 1 window with token length of N\n",
        "        flops = 0\n",
        "        # qkv = self.qkv(x)\n",
        "        flops += N * self.dim * 3 * self.dim\n",
        "        # attn = (q @ k.transpose(-2, -1))\n",
        "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
        "        #  x = (attn @ v)\n",
        "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
        "        # x = self.proj(x)\n",
        "        flops += N * self.dim * self.dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\" Swin Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            attn_mask = self.calculate_mask(self.input_resolution)\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def calculate_mask(self, x_size):\n",
        "        # calculate attention mask for SW-MSA\n",
        "        H, W = x_size\n",
        "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        H, W = x_size\n",
        "        B, L, C = x.shape\n",
        "        # assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
        "        if self.input_resolution == x_size:\n",
        "            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
        "        else:\n",
        "            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        nW = H * W / self.window_size / self.window_size\n",
        "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
        "        # mlp\n",
        "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    r\"\"\" Patch Merging Layer.\n",
        "\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
        "\n",
        "    def flops(self):\n",
        "        H, W = self.input_resolution\n",
        "        flops = H * W * self.dim\n",
        "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, window_size=window_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        # patch merging layer\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x, x_size)\n",
        "            else:\n",
        "                x = blk(x, x_size)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        if self.downsample is not None:\n",
        "            flops += self.downsample.flops()\n",
        "        return flops\n",
        "\n",
        "\n",
        "class RSTB(nn.Module):\n",
        "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "        img_size: Input image size.\n",
        "        patch_size: Patch size.\n",
        "        resi_connection: The convolutional block before residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
        "                 img_size=224, patch_size=4, resi_connection='1conv'):\n",
        "        super(RSTB, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        self.residual_group = BasicLayer(dim=dim,\n",
        "                                         input_resolution=input_resolution,\n",
        "                                         depth=depth,\n",
        "                                         num_heads=num_heads,\n",
        "                                         window_size=window_size,\n",
        "                                         mlp_ratio=mlp_ratio,\n",
        "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                         drop=drop, attn_drop=attn_drop,\n",
        "                                         drop_path=drop_path,\n",
        "                                         norm_layer=norm_layer,\n",
        "                                         downsample=downsample,\n",
        "                                         use_checkpoint=use_checkpoint)\n",
        "\n",
        "        if resi_connection == '1conv':\n",
        "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
        "        elif resi_connection == '3conv':\n",
        "            # to save parameters and memory\n",
        "            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n",
        "                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
        "            norm_layer=None)\n",
        "\n",
        "        self.patch_unembed = PatchUnEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
        "            norm_layer=None)\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        flops += self.residual_group.flops()\n",
        "        H, W = self.input_resolution\n",
        "        flops += H * W * self.dim * self.dim * 9\n",
        "        flops += self.patch_embed.flops()\n",
        "        flops += self.patch_unembed.flops()\n",
        "\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    r\"\"\" Image to Patch Embedding\n",
        "\n",
        "    Args:\n",
        "        img_size (int): Image size.  Default: 224.\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.img_size\n",
        "        if self.norm is not None:\n",
        "            flops += H * W * self.embed_dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchUnEmbed(nn.Module):\n",
        "    r\"\"\" Image to Patch Unembedding\n",
        "\n",
        "    Args:\n",
        "        img_size (int): Image size.  Default: 224.\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        B, HW, C = x.shape\n",
        "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        return flops\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    \"\"\"Upsample module.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat):\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "        else:\n",
        "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
        "        super(Upsample, self).__init__(*m)\n",
        "\n",
        "\n",
        "class UpsampleOneStep(nn.Sequential):\n",
        "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
        "       Used in lightweight SR to save parameters.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
        "        self.num_feat = num_feat\n",
        "        self.input_resolution = input_resolution\n",
        "        m = []\n",
        "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
        "        m.append(nn.PixelShuffle(scale))\n",
        "        super(UpsampleOneStep, self).__init__(*m)\n",
        "\n",
        "    def flops(self):\n",
        "        H, W = self.input_resolution\n",
        "        flops = H * W * self.num_feat * 3 * 9\n",
        "        return flops\n",
        "\n",
        "\n",
        "class SwinIR(nn.Module):\n",
        "    r\"\"\" SwinIR\n",
        "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
        "\n",
        "    Args:\n",
        "        img_size (int | tuple(int)): Input image size. Default 64\n",
        "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        embed_dim (int): Patch embedding dimension. Default: 96\n",
        "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
        "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
        "        window_size (int): Window size. Default: 7\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
        "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
        "        drop_rate (float): Dropout rate. Default: 0\n",
        "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
        "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
        "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
        "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
        "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
        "        img_range: Image range. 1. or 255.\n",
        "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
        "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n",
        "                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n",
        "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
        "                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n",
        "                 **kwargs):\n",
        "        super(SwinIR, self).__init__()\n",
        "        num_in_ch = in_chans\n",
        "        num_out_ch = in_chans\n",
        "        num_feat = 64\n",
        "        self.img_range = img_range\n",
        "        if in_chans == 3:\n",
        "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
        "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
        "        else:\n",
        "            self.mean = torch.zeros(1, 1, 1, 1)\n",
        "        self.upscale = upscale\n",
        "        self.upsampler = upsampler\n",
        "        self.window_size = window_size\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################### 1, shallow feature extraction ###################################\n",
        "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################### 2, deep feature extraction ######################################\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = embed_dim\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        # merge non-overlapping patches into image\n",
        "        self.patch_unembed = PatchUnEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None)\n",
        "\n",
        "        # absolute position embedding\n",
        "        if self.ape:\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        # build Residual Swin Transformer blocks (RSTB)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = RSTB(dim=embed_dim,\n",
        "                         input_resolution=(patches_resolution[0],\n",
        "                                           patches_resolution[1]),\n",
        "                         depth=depths[i_layer],\n",
        "                         num_heads=num_heads[i_layer],\n",
        "                         window_size=window_size,\n",
        "                         mlp_ratio=self.mlp_ratio,\n",
        "                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
        "                         norm_layer=norm_layer,\n",
        "                         downsample=None,\n",
        "                         use_checkpoint=use_checkpoint,\n",
        "                         img_size=img_size,\n",
        "                         patch_size=patch_size,\n",
        "                         resi_connection=resi_connection\n",
        "\n",
        "                         )\n",
        "            self.layers.append(layer)\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "\n",
        "        # build the last conv layer in deep feature extraction\n",
        "        if resi_connection == '1conv':\n",
        "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
        "        elif resi_connection == '3conv':\n",
        "            # to save parameters and memory\n",
        "            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n",
        "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n",
        "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################ 3, high quality image reconstruction ################################\n",
        "        if self.upsampler == 'pixelshuffle':\n",
        "            # for classical SR\n",
        "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
        "                                                      nn.LeakyReLU(inplace=True))\n",
        "            self.upsample = Upsample(upscale, num_feat)\n",
        "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
        "        elif self.upsampler == 'pixelshuffledirect':\n",
        "            # for lightweight SR (to save parameters)\n",
        "            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n",
        "                                            (patches_resolution[0], patches_resolution[1]))\n",
        "        elif self.upsampler == 'nearest+conv':\n",
        "            # for real-world SR (less artifacts)\n",
        "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
        "                                                      nn.LeakyReLU(inplace=True))\n",
        "            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            if self.upscale == 4:\n",
        "                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
        "            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        else:\n",
        "            # for image denoising and JPEG compression artifact reduction\n",
        "            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
        "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
        "        return x\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x_size = (x.shape[2], x.shape[3])\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x_size)\n",
        "\n",
        "        x = self.norm(x)  # B L C\n",
        "        x = self.patch_unembed(x, x_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.shape[2:]\n",
        "        x = self.check_image_size(x)\n",
        "\n",
        "        self.mean = self.mean.type_as(x)\n",
        "        x = (x - self.mean) * self.img_range\n",
        "\n",
        "        if self.upsampler == 'pixelshuffle':\n",
        "            # for classical SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.conv_before_upsample(x)\n",
        "            x = self.conv_last(self.upsample(x))\n",
        "        elif self.upsampler == 'pixelshuffledirect':\n",
        "            # for lightweight SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.upsample(x)\n",
        "        elif self.upsampler == 'nearest+conv':\n",
        "            # for real-world SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.conv_before_upsample(x)\n",
        "            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "            if self.upscale == 4:\n",
        "                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n",
        "        else:\n",
        "            # for image denoising and JPEG compression artifact reduction\n",
        "            x_first = self.conv_first(x)\n",
        "            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n",
        "            x = x + self.conv_last(res)\n",
        "\n",
        "        x = x / self.img_range + self.mean\n",
        "\n",
        "        return x[:, :, :H*self.upscale, :W*self.upscale]\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.patches_resolution\n",
        "        flops += H * W * 3 * self.embed_dim * 9\n",
        "        flops += self.patch_embed.flops()\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            flops += layer.flops()\n",
        "        flops += H * W * 3 * self.embed_dim * self.embed_dim\n",
        "        flops += self.upsample.flops()\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCKdjwUaH1oc"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
        "        self.lr_dir = lr_dir\n",
        "        self.hr_dir = hr_dir\n",
        "        self.transform = transform\n",
        "        self.filenames = [name for name in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, name))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr_path = os.path.join(self.lr_dir, self.filenames[idx])\n",
        "        hr_path = os.path.join(self.hr_dir, self.filenames[idx].replace('slice_', 'slice_'))\n",
        "\n",
        "        lr_image = Image.open(lr_path).convert('RGB')\n",
        "        hr_image = Image.open(hr_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            lr_image = self.transform(lr_image)\n",
        "            hr_image = self.transform(hr_image)\n",
        "\n",
        "        return lr_image, hr_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_9hT9RxE125"
      },
      "outputs": [],
      "source": [
        "class SharpenTransform:\n",
        "    def __init__(self, radius=2, percent=50, threshold=3):\n",
        "        self.radius = radius\n",
        "        self.percent = percent\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return img.filter(ImageFilter.UnsharpMask(\n",
        "            radius=self.radius,\n",
        "            percent=self.percent,\n",
        "            threshold=self.threshold\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FgKHfvIfsDn"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_transform = Compose([\n",
        "    RandomHorizontalFlip(p=0.2),\n",
        "    GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 1.5)),\n",
        "    SharpenTransform(radius=2, percent=50, threshold=3),\n",
        "    ToTensor(),\n",
        "    Resize((64, 64)),\n",
        "])\n",
        "\n",
        "full_dataset = SuperResolutionDataset(\n",
        "    lr_dir='/content/drive/MyDrive/FYP/dataset3/reconstruction_esc',\n",
        "    hr_dir='/content/drive/MyDrive/FYP/dataset3/reconstruction_rss',\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "indices = torch.randperm(len(full_dataset)).tolist()\n",
        "\n",
        "train_indices, eval_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "eval_dataset = Subset(full_dataset, eval_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=20, shuffle=False)\n",
        "eval_loader = DataLoader(dataset=eval_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM-9j8U5H1oc",
        "outputId": "1c638d63-091e-49f4-f034-1622c3c676ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the SwinIR model\n",
        "model = SwinIR(upscale=2, in_chans=3, img_size=64, window_size=8, img_range=1.,\n",
        "               depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
        "               mlp_ratio=2, upsampler='pixelshuffle', resi_connection='3conv')\n",
        "\n",
        "# Load the pre-trained weights\n",
        "#param_key_g = \"params\"\n",
        "#pretrained_dict = torch.load(\"/content/drive/MyDrive/FYP/swin-ir.pth\")\n",
        "#model.load_state_dict(pretrained_dict[param_key_g] if param_key_g in pretrained_dict.keys() else pretrained_dict, strict=True)\n",
        "#model.load_state_dict(torch.load('swin-ir.pth'))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "# Define a loss function\n",
        "loss_fn = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XyBR7V4bfsDp",
        "outputId": "dd78453d-09b5-4c67-fecd-e9b0777b70dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Training Loss: 0.0357, Evaluation Loss: 0.0298\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [2/20], Training Loss: 0.0290, Evaluation Loss: 0.0288\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [3/20], Training Loss: 0.0281, Evaluation Loss: 0.0277\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [4/20], Training Loss: 0.0272, Evaluation Loss: 0.0270\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [5/20], Training Loss: 0.0270, Evaluation Loss: 0.0267\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [6/20], Training Loss: 0.0270, Evaluation Loss: 0.0266\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [7/20], Training Loss: 0.0271, Evaluation Loss: 0.0261\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [8/20], Training Loss: 0.0263, Evaluation Loss: 0.0265\n",
            "Validation loss did not decrease, count: 1\n",
            "Epoch [9/20], Training Loss: 0.0267, Evaluation Loss: 0.0262\n",
            "Validation loss did not decrease, count: 2\n",
            "Epoch [10/20], Training Loss: 0.0263, Evaluation Loss: 0.0264\n",
            "Validation loss did not decrease, count: 3\n",
            "Epoch [11/20], Training Loss: 0.0264, Evaluation Loss: 0.0265\n",
            "Validation loss did not decrease, count: 4\n",
            "Epoch [12/20], Training Loss: 0.0267, Evaluation Loss: 0.0254\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [13/20], Training Loss: 0.0259, Evaluation Loss: 0.0258\n",
            "Validation loss did not decrease, count: 1\n",
            "Epoch [14/20], Training Loss: 0.0256, Evaluation Loss: 0.0258\n",
            "Validation loss did not decrease, count: 2\n",
            "Epoch [15/20], Training Loss: 0.0255, Evaluation Loss: 0.0252\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [16/20], Training Loss: 0.0256, Evaluation Loss: 0.0256\n",
            "Validation loss did not decrease, count: 1\n",
            "Epoch [17/20], Training Loss: 0.0254, Evaluation Loss: 0.0253\n",
            "Validation loss did not decrease, count: 2\n",
            "Epoch [18/20], Training Loss: 0.0257, Evaluation Loss: 0.0252\n",
            "Validation loss did not decrease, count: 3\n",
            "Epoch [19/20], Training Loss: 0.0253, Evaluation Loss: 0.0245\n",
            "Validation loss decreased, saving best model...\n",
            "Epoch [20/20], Training Loss: 0.0255, Evaluation Loss: 0.0247\n",
            "Validation loss did not decrease, count: 1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize early stopping parameters\n",
        "best_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "early_stopping_patience = 5\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop parameters\n",
        "num_epochs = 20\n",
        "train_loss_per_epoch = []\n",
        "eval_loss_per_epoch = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_eval_loss = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for lr_imgs, hr_imgs in train_loader:\n",
        "        lr_imgs = lr_imgs.to(device)\n",
        "        hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        sr_imgs = model(lr_imgs)\n",
        "        # Resize hr_imgs to match the output of the model (sr_imgs)\n",
        "        hr_imgs = F.interpolate(hr_imgs, size=sr_imgs.shape[2:], mode='bicubic', align_corners=False)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(sr_imgs, hr_imgs)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute average training loss\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for lr_imgs, hr_imgs in eval_loader:\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            sr_imgs = model(lr_imgs)\n",
        "            # Resize hr_imgs to match sr_imgs size before loss calculation\n",
        "            hr_imgs = F.interpolate(hr_imgs, size=sr_imgs.shape[2:], mode='bicubic', align_corners=False)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(sr_imgs, hr_imgs)\n",
        "            running_eval_loss += loss.item()\n",
        "\n",
        "    # Compute average evaluation loss\n",
        "    avg_eval_loss = running_eval_loss / len(eval_loader)\n",
        "    eval_loss_per_epoch.append(avg_eval_loss)\n",
        "\n",
        "    # Print training and evaluation loss\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Evaluation Loss: {avg_eval_loss:.4f}')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if avg_eval_loss < best_loss:\n",
        "        best_loss = avg_eval_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "        print('Validation loss decreased, saving best model...')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f'Validation loss did not decrease, count: {epochs_no_improve}')\n",
        "\n",
        "    if epochs_no_improve >= early_stopping_patience:\n",
        "        print('Early stopping triggered.')\n",
        "        break\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# Save the best model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/FYP/weights9.pth')\n",
        "\n",
        "# Plot the training and evaluation losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_per_epoch, label='Training Loss')\n",
        "plt.plot(eval_loss_per_epoch, label='Evaluation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
